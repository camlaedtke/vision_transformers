{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d632f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from vit_keras import vit, utils, visualize\n",
    "from models.clf.vit import VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7efd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 768\n",
    "patch_size = 16\n",
    "n_classes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dropout\": 0.1,\n",
    "    \"mlp_dim\": 4096,\n",
    "    \"num_heads\": 16,\n",
    "    \"num_layers\": 24,\n",
    "    \"hidden_size\": 1024,\n",
    "    \"name\": \"vit-l16\",\n",
    "    \"pretrained\": \"weights/vit_l16_imagenet21k_imagenet2012.h5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VIT(\n",
    "#     image_size=img_size, \n",
    "#     patch_size=patch_size, \n",
    "#     num_classes=n_classes, \n",
    "#     num_layers=config[\"num_layers\"], \n",
    "#     hidden_size=config[\"hidden_size\"], \n",
    "#     mlp_dim=config[\"mlp_dim\"], \n",
    "#     num_heads=config[\"num_heads\"], \n",
    "#     name=config[\"name\"], \n",
    "#     dropout=config[\"dropout\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = utils.get_imagenet_classes()\n",
    "model = vit.vit_l16(\n",
    "    image_size=img_size,\n",
    "    activation='sigmoid',\n",
    "    pretrained=True,\n",
    "    include_top=True,\n",
    "    pretrained_top=True,\n",
    "    weights=\"imagenet21k+imagenet2012\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb73e99",
   "metadata": {},
   "source": [
    "```python\n",
    "Model: \"VIT-L_16\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 384, 384, 3)]     0         \n",
    "_________________________________________________________________\n",
    "embedding (Conv2D)           (None, 24, 24, 1024)      787456    \n",
    "_________________________________________________________________\n",
    "reshape (Reshape)            (None, 576, 1024)         0         \n",
    "_________________________________________________________________\n",
    "class_token (ClassToken)     (None, 577, 1024)         1024      \n",
    "_________________________________________________________________\n",
    "Transformer/posembed_input ( (None, 577, 1024)         590848    \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_0 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_1 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_2 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_3 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_4 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_5 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_6 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_7 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_8 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_9 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_10  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_11  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_12  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_13  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_14  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_15  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_16  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_17  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_18  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_19  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_20  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_21  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_22  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_23  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoder_norm (La (None, 577, 1024)         2048      \n",
    "_________________________________________________________________\n",
    "ExtractToken (Lambda)        (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "head (Dense)                 (None, 1000)              1025000   \n",
    "=================================================================\n",
    "Total params: 304,715,752\n",
    "Trainable params: 304,715,752\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "Model: \"vit-l16\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 384, 384, 3)]     0         \n",
    "_________________________________________________________________\n",
    "embedding (Conv2D)           (None, 24, 24, 1024)      787456    \n",
    "_________________________________________________________________\n",
    "reshape (Reshape)            (None, 576, 1024)         0         \n",
    "_________________________________________________________________\n",
    "class_token (ClassToken)     (None, 577, 1024)         1024      \n",
    "_________________________________________________________________\n",
    "Transformer/posembed_input ( (None, 577, 1024)         590848    \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_0 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_1 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_2 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_3 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_4 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_5 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_6 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_7 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_8 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_9 ( ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_10  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_11  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_12  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_13  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_14  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_15  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_16  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_17  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_18  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_19  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_20  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_21  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_22  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoderblock_23  ((None, 577, 1024), (None 12596224  \n",
    "_________________________________________________________________\n",
    "Transformer/encoder_norm (La (None, 577, 1024)         2048      \n",
    "_________________________________________________________________\n",
    "ExtractToken (Lambda)        (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "head (Dense)                 (None, 1000)              1025000   \n",
    "=================================================================\n",
    "Total params: 304,715,752\n",
    "Trainable params: 304,715,752\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62405ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights/vit_l16_imagenet21k_imagenet2012.h5\")\n",
    "model.save_weights(\"weights/vit_l16_imagenet21k_imagenet2012.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2929f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://upload.wikimedia.org/wikipedia/commons/d/d7/Granny_smith_and_cross_section.jpg'\n",
    "image = utils.read(url, img_size)\n",
    "X = vit.preprocess_inputs(image).reshape(1, img_size, img_size, 3)\n",
    "y = model.predict(X)\n",
    "print(classes[y[0].argmax()]) # Granny smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image and compute the attention map\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/b/bc/Free%21_%283987584939%29.jpg'\n",
    "image = utils.read(url, image_size)\n",
    "attention_map = visualize.attention_map(model=model, image=image)\n",
    "print('Prediction:', classes[model.predict(vit.preprocess_inputs(image)[np.newaxis])[0].argmax()])  \n",
    "# Prediction: Eskimo dog, husky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16,8))\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(image)\n",
    "_ = ax2.imshow(attention_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e542b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
