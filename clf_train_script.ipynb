{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93925f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay, PiecewiseConstantDecay\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from models.clf.vit import VIT, TransformerBlock\n",
    "from vit_keras import vit, layers, utils\n",
    "from utils.train_utils import TrainAccumilatorCLF\n",
    "\n",
    "K.clear_session()\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "def enable_amp():\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(physical_devices,\"\\n\")\n",
    "enable_amp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_map(model, image):\n",
    "    \"\"\"Get an attention map for an image and model using the technique\n",
    "    described in Appendix D.7 in the paper (unofficial).\n",
    "    Args:\n",
    "        model: A ViT model\n",
    "        image: An image for which we will compute the attention map.\n",
    "    \"\"\"\n",
    "    size = model.input_shape[1]\n",
    "    grid_size = int(np.sqrt(model.layers[5].output_shape[0][-2] - 1))\n",
    "\n",
    "    # Prepare the input\n",
    "    X = vit.preprocess_inputs(cv2.resize(image, (size, size)))[np.newaxis, :]  # type: ignore\n",
    "\n",
    "    # Get the attention weights from each transformer.\n",
    "    outputs = [\n",
    "        l.output[1] for l in model.layers if isinstance(l, TransformerBlock)\n",
    "    ]\n",
    "    weights = np.array(\n",
    "        tf.keras.models.Model(inputs=model.inputs, outputs=outputs).predict(X)\n",
    "    )\n",
    "    num_layers = weights.shape[0]\n",
    "    num_heads = weights.shape[2]\n",
    "    reshaped = weights.reshape(\n",
    "        (num_layers, num_heads, grid_size ** 2 + 1, grid_size ** 2 + 1)\n",
    "    )\n",
    "\n",
    "    # From Appendix D.6 in the paper ...\n",
    "    # Average the attention weights across all heads.\n",
    "    reshaped = reshaped.mean(axis=1)\n",
    "\n",
    "    # From Section 3 in https://arxiv.org/pdf/2005.00928.pdf ...\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    reshaped = reshaped + np.eye(reshaped.shape[1])\n",
    "    reshaped = reshaped / reshaped.sum(axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    v = reshaped[-1]\n",
    "    for n in range(1, len(reshaped)):\n",
    "        v = np.matmul(v, reshaped[-1 - n])\n",
    "\n",
    "    # Attention from the output token to the input space.\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size)\n",
    "    mask = cv2.resize(mask / mask.max(), (image.shape[1], image.shape[0]))[\n",
    "        ..., np.newaxis\n",
    "    ]\n",
    "    return (mask * image).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetLoader():\n",
    "    \n",
    "    def __init__(self, img_height, img_width, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.MEAN = np.array([0.485, 0.456, 0.406])\n",
    "        self.STD = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def random_crop(self, image):\n",
    "\n",
    "        scales = tf.convert_to_tensor(np.array([0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0]))\n",
    "        scale = scales[tf.random.uniform(shape=[], minval=0, maxval=8, dtype=tf.int32)]\n",
    "        scale = tf.cast(scale, tf.float32)\n",
    "\n",
    "        shape = tf.cast(tf.shape(image), tf.float32)\n",
    "        h = tf.cast(shape[0] * scale, tf.int32)\n",
    "        w = tf.cast(shape[1] * scale, tf.int32)\n",
    "        image = tf.image.random_crop(image, size=[h, w, 3])\n",
    "        return image\n",
    "\n",
    "    @tf.function\n",
    "    def normalize(self, image):\n",
    "        image = image / 255.0\n",
    "        image = image - self.MEAN\n",
    "        image = image / self.STD\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def load_image_train(self, datapoint):\n",
    "\n",
    "        img = datapoint['image']\n",
    "        label = datapoint['label']\n",
    "        label = tf.one_hot(tf.cast(label, tf.int32), self.n_classes)\n",
    "\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.flip_left_right(img)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.random_brightness(img, 0.1)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.random_contrast(img, 0.7, 1.3)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "        img = self.random_crop(img)\n",
    "        img = tf.image.resize(img, (self.img_height, self.img_width), method='bilinear')\n",
    "        img = self.normalize(tf.cast(img, tf.float32))\n",
    "\n",
    "        return img, label\n",
    "   \n",
    "\n",
    "    def load_image_test(self, datapoint):\n",
    "        img = datapoint['image']\n",
    "        label = datapoint['label']\n",
    "        label = tf.one_hot(tf.cast(label, tf.int32), self.n_classes)\n",
    "        img = tf.image.resize(img, (self.img_height, self.img_width), method='bilinear')\n",
    "        img = self.normalize(tf.cast(img, tf.float32))\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50609f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 768\n",
    "patch_size = 16\n",
    "n_classes = 1000\n",
    "img_width = img_size\n",
    "img_height = img_size\n",
    "classes = utils.get_imagenet_classes()\n",
    "\n",
    "pipeline = ImageNetLoader(\n",
    "    n_classes = n_classes,\n",
    "    img_height = img_height,\n",
    "    img_width = img_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\n",
    "    name='imagenet2012:5.0.0', \n",
    "    data_dir='/workspace/tensorflow_datasets/', \n",
    "    with_info=True, \n",
    "    shuffle_files=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617432fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train'].map(pipeline.load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid = dataset['validation'].map(pipeline.load_image_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "VALID_LENGTH = info.splits['validation'].num_examples\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "ACCUM_STEPS = 128\n",
    "BUFFER_SIZE = 512\n",
    "ADJ_BATCH_SIZE = BATCH_SIZE * ACCUM_STEPS\n",
    "print(\"Effective batch size: {}\".format(ADJ_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ec29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img, true_label, pred_label=None):\n",
    "    plt.figure(figsize=(6,6), dpi=120)\n",
    "    plt.title(\"True label: {}\".format(true_label), fontsize=12)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(img))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train.take(4): \n",
    "    sample_image, sample_label = image, label\n",
    "\n",
    "# print(sample_image.shape, sample_label.shape)\n",
    "display_img(img=sample_image, true_label=classes[tf.argmax(sample_label).numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dropout\": 0.1,\n",
    "    \"mlp_dim\": 4096,\n",
    "    \"num_heads\": 16,\n",
    "    \"num_layers\": 24,\n",
    "    \"hidden_size\": 1024,\n",
    "    \"name\": \"vit-l16\",\n",
    "    \"pretrained\": \"weights/vit_l16_imagenet21k_imagenet2012.h5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VIT(\n",
    "    image_size=img_size, \n",
    "    patch_size=patch_size, \n",
    "    num_classes=n_classes, \n",
    "    num_layers=config[\"num_layers\"], \n",
    "    hidden_size=config[\"hidden_size\"], \n",
    "    mlp_dim=config[\"mlp_dim\"], \n",
    "    num_heads=config[\"num_heads\"], \n",
    "    name=config[\"name\"], \n",
    "    dropout=config[\"dropout\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(config[\"pretrained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attn(img):\n",
    "    attn = attention_map(model=model, image=img.numpy())\n",
    "    print('Prediction:', classes[model.predict(img[tf.newaxis, ...])[0].argmax()]) \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16,8))\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax1.set_title('Original')\n",
    "    ax2.set_title('Attention Map')\n",
    "    _ = ax1.imshow(tf.keras.preprocessing.image.array_to_img(img))\n",
    "    _ = ax2.imshow(tf.keras.preprocessing.image.array_to_img(attn))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a09fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_attn(img=sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf07cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"weights/\"+model.name+\".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // ADJ_BATCH_SIZE\n",
    "VALIDATION_STEPS = VALID_LENGTH // BATCH_SIZE\n",
    "DECAY_STEPS = (STEPS_PER_EPOCH * EPOCHS) \n",
    "print(\"Decay steps: {}\".format(DECAY_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3509841",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_EPOCH = 0\n",
    "E1 = 30 - CURR_EPOCH\n",
    "E2 = 60 - CURR_EPOCH\n",
    "E3 = 90 - CURR_EPOCH\n",
    "\n",
    "S1 = (STEPS_PER_EPOCH * E1) // ACCUM_STEPS\n",
    "S2 = (STEPS_PER_EPOCH * E2) // ACCUM_STEPS\n",
    "S3 = (STEPS_PER_EPOCH * E3) // ACCUM_STEPS\n",
    "\n",
    "print(\"--- LR decay --- \\nstep {}: {} \\nstep {}: {} \\nstep {}: {}\".format(S1, 1e-2, S2, 1e-3, S3, 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f50607",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = PiecewiseConstantDecay(boundaries = [S1, S2, S3], values = [0.1, 0.01, 0.001, 0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236479df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "opt = SGD(learning_rate=1e-3, momentum=0.9)\n",
    "\n",
    "trainer = TrainAccumilatorCLF(\n",
    "    model = model,\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(opt),\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    n_classes = n_classes,\n",
    "    reduce_lr_on_plateau = None,\n",
    "    accum_steps = ACCUM_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e511a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = trainer.fit(\n",
    "    epochs = EPOCHS,\n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = valid_dataset, \n",
    "    weights_path = MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65495dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(results, model):\n",
    "         \n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(1,3,1)  \n",
    "\n",
    "    plt.plot(results.history['loss'], 'r', label='Training loss')\n",
    "    plt.plot(results.history['val_loss'], 'b', label='Validation loss')\n",
    "    plt.title(\"Loss: \"+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(results.history['accuracy'], 'r', label='Training accuracy')\n",
    "    plt.plot(results.history['val_accuracy'], 'b', label='Validation accuracy')\n",
    "    plt.title('Accuracy: '+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(results.history['iou_coef'], 'r', label='IoU coefficient')\n",
    "    plt.plot(results.history['val_iou_coef'], 'b', label='Validation IoU coefficient')\n",
    "    plt.title('IoU Coefficient: '+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    \n",
    "    if fine:\n",
    "        plt.savefig(\"plots/\"+model.name+\"_learning_curves.png\")\n",
    "    else:\n",
    "        plt.savefig(\"plots/\"+model.name+\"_learning_curves_coarse.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18190bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(results, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ae213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd84c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
