{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import typing\n",
    "import warnings\n",
    "# import validators\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pkg_resources\n",
    "from http import client\n",
    "import tensorflow as tf\n",
    "from urllib import request\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "try:\n",
    "    import PIL\n",
    "    import PIL.Image\n",
    "except ImportError:  # pragma: no cover\n",
    "    PIL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassToken(tf.keras.layers.Layer):\n",
    "    \"\"\"Append a class token to an input layer.\"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        cls_init = tf.zeros_initializer()\n",
    "        self.hidden_size = input_shape[-1]\n",
    "        self.cls = tf.Variable(\n",
    "            name=\"cls\",\n",
    "            initial_value=cls_init(shape=(1, 1, self.hidden_size), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        cls_broadcasted = tf.cast(\n",
    "            tf.broadcast_to(self.cls, [batch_size, 1, self.hidden_size]),\n",
    "            dtype=inputs.dtype,\n",
    "        )\n",
    "        return tf.concat([cls_broadcasted, inputs], 1)\n",
    "\n",
    "\n",
    "class AddPositionEmbs(tf.keras.layers.Layer):\n",
    "    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert (\n",
    "            len(input_shape) == 3\n",
    "        ), f\"Number of dimensions should be 3, got {len(input_shape)}\"\n",
    "        self.pe = tf.Variable(\n",
    "            name=\"pos_embedding\",\n",
    "            initial_value=tf.random_normal_initializer(stddev=0.06)(\n",
    "                shape=(1, input_shape[1], input_shape[2])\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + tf.cast(self.pe, dtype=inputs.dtype)\n",
    "    \n",
    "    \n",
    "    \n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, num_heads, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        hidden_size = input_shape[-1]\n",
    "        num_heads = self.num_heads\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {hidden_size} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection_dim = hidden_size // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(hidden_size, name=\"query\")\n",
    "        self.key_dense = tf.keras.layers.Dense(hidden_size, name=\"key\")\n",
    "        self.value_dense = tf.keras.layers.Dense(hidden_size, name=\"value\")\n",
    "        self.combine_heads = tf.keras.layers.Dense(hidden_size, name=\"out\")\n",
    "\n",
    "    # pylint: disable=no-self-use\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], score.dtype)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output, weights\n",
    "    \n",
    "    \n",
    "\n",
    "# pylint: disable=too-many-instance-attributes\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Implements a Transformer block.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, num_heads, mlp_dim, dropout, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.att = MultiHeadSelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            name=\"MultiHeadDotProductAttention_1\",\n",
    "        )\n",
    "        self.mlpblock = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(\n",
    "                    self.mlp_dim,\n",
    "                    activation=\"linear\",\n",
    "                    name=f\"{self.name}/Dense_0\",\n",
    "                ),\n",
    "                tf.keras.layers.Lambda(\n",
    "                    lambda x: tf.keras.activations.gelu(x, approximate=False)\n",
    "                )\n",
    "                if hasattr(tf.keras.activations, \"gelu\")\n",
    "                else tf.keras.layers.Lambda(\n",
    "                    lambda x: tfa.activations.gelu(x, approximate=False)\n",
    "                ),\n",
    "                tf.keras.layers.Dropout(self.dropout),\n",
    "                tf.keras.layers.Dense(input_shape[-1], name=f\"{self.name}/Dense_1\"),\n",
    "                tf.keras.layers.Dropout(self.dropout),\n",
    "            ],\n",
    "            name=\"MlpBlock_3\",\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"LayerNorm_0\"\n",
    "        )\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"LayerNorm_2\"\n",
    "        )\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x, weights = self.att(x)\n",
    "        x = self.dropout_layer(x, training=training)\n",
    "        x = x + inputs\n",
    "        y = self.layernorm2(x)\n",
    "        y = self.mlpblock(y)\n",
    "        return x + y, weights\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"mlp_dim\": self.mlp_dim,\n",
    "            \"dropout\": self.dropout,\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def get_imagenet_classes() -> typing.List[str]:\n",
    "    \"\"\"Get the list of ImageNet 2012 classes.\"\"\"\n",
    "    filepath = pkg_resources.resource_filename(\"vit_keras\", \"imagenet2012.txt\")\n",
    "    with open(filepath) as f:\n",
    "        classes = [l.strip() for l in f.readlines()]\n",
    "    return classes\n",
    "\n",
    "\n",
    "def apply_embedding_weights(target_layer, source_weights):\n",
    "    \"\"\"Apply embedding weights to a target layer.\n",
    "\n",
    "    Args:\n",
    "        target_layer: The target layer to which weights will be applied.\n",
    "        source_weights: The source weights, which will be resized as necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    expected_shape = target_layer.weights[0].shape\n",
    "    print(\"target shape: \", expected_shape, \"source shape: \", source_weights.shape)\n",
    "    if expected_shape != source_weights.shape:\n",
    "        token, grid = source_weights[0, :1], source_weights[0, 1:]\n",
    "        sin = int(np.sqrt(grid.shape[0]))\n",
    "        sout = int(np.sqrt(expected_shape[1] - 1))\n",
    "        warnings.warn(\n",
    "            \"Resizing position embeddings from \" f\"{sin} to {sout}\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        zoom = (sout / sin, sout / sin, 1)\n",
    "        grid = sp.ndimage.zoom(grid.reshape(sin, sin, -1), zoom, order=1).reshape(\n",
    "            sout * sout, -1\n",
    "        )\n",
    "        source_weights = np.concatenate([token, grid], axis=0)[np.newaxis]\n",
    "    target_layer.set_weights([source_weights])\n",
    "    \n",
    "    \n",
    "def load_weights_numpy(model, params_path, pretrained_top):\n",
    "    \"\"\"Load weights saved using Flax as a numpy array.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras model to load the weights into.\n",
    "        params_path: Filepath to a numpy archive.\n",
    "        pretrained_top: Whether to load the top layer weights.\n",
    "    \"\"\"\n",
    "    params_dict = np.load(params_path, allow_pickle=False) \n",
    "    source_keys = list(params_dict.keys())\n",
    "    pre_logits = any(l.name == \"pre_logits\" for l in model.layers)\n",
    "    source_keys_used = []\n",
    "    n_transformers = len(\n",
    "        set(\n",
    "            \"/\".join(k.split(\"/\")[:2])\n",
    "            for k in source_keys\n",
    "            if k.startswith(\"Transformer/encoderblock_\")\n",
    "        )\n",
    "    )\n",
    "    n_transformers_out = sum(\n",
    "        l.name.startswith(\"Transformer/encoderblock_\") for l in model.layers\n",
    "    )\n",
    "    assert n_transformers == n_transformers_out, (\n",
    "        f\"Wrong number of transformers (\"\n",
    "        f\"{n_transformers_out} in model vs. {n_transformers} in weights).\"\n",
    "    )\n",
    "\n",
    "    matches = []\n",
    "    for tidx in range(n_transformers):\n",
    "        encoder = model.get_layer(f\"Transformer/encoderblock_{tidx}\")\n",
    "        source_prefix = f\"Transformer/encoderblock_{tidx}\"\n",
    "        matches.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"layer\": layer,\n",
    "                    \"keys\": [\n",
    "                        f\"{source_prefix}/{norm}/{name}\" for name in [\"scale\", \"bias\"]\n",
    "                    ],\n",
    "                }\n",
    "                for norm, layer in [\n",
    "                    (\"LayerNorm_0\", encoder.layernorm1),\n",
    "                    (\"LayerNorm_2\", encoder.layernorm2),\n",
    "                ]\n",
    "            ]\n",
    "            + [\n",
    "                {\n",
    "                    \"layer\": encoder.mlpblock.get_layer(\n",
    "                        f\"{source_prefix}/Dense_{mlpdense}\"\n",
    "                    ),\n",
    "                    \"keys\": [\n",
    "                        f\"{source_prefix}/MlpBlock_3/Dense_{mlpdense}/{name}\"\n",
    "                        for name in [\"kernel\", \"bias\"]\n",
    "                    ],\n",
    "                }\n",
    "                for mlpdense in [0, 1]\n",
    "            ]\n",
    "            + [\n",
    "                {\n",
    "                    \"layer\": layer,\n",
    "                    \"keys\": [\n",
    "                        f\"{source_prefix}/MultiHeadDotProductAttention_1/{attvar}/{name}\"\n",
    "                        for name in [\"kernel\", \"bias\"]\n",
    "                    ],\n",
    "                    \"reshape\": True,\n",
    "                }\n",
    "                for attvar, layer in [\n",
    "                    (\"query\", encoder.att.query_dense),\n",
    "                    (\"key\", encoder.att.key_dense),\n",
    "                    (\"value\", encoder.att.value_dense),\n",
    "                    (\"out\", encoder.att.combine_heads),\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    for layer_name in [\"embedding\", \"head\", \"pre_logits\"]:\n",
    "        if layer_name == \"head\" and not pretrained_top:\n",
    "            source_keys_used.extend([\"head/kernel\", \"head/bias\"])\n",
    "            continue\n",
    "        if layer_name == \"pre_logits\" and not pre_logits:\n",
    "            continue\n",
    "        matches.append(\n",
    "            {\n",
    "                \"layer\": model.get_layer(layer_name),\n",
    "                \"keys\": [f\"{layer_name}/{name}\" for name in [\"kernel\", \"bias\"]],\n",
    "            }\n",
    "        )\n",
    "    matches.append({\"layer\": model.get_layer(\"class_token\"), \"keys\": [\"cls\"]})\n",
    "    matches.append(\n",
    "        {\n",
    "            \"layer\": model.get_layer(\"Transformer/encoder_norm\"),\n",
    "            \"keys\": [f\"Transformer/encoder_norm/{name}\" for name in [\"scale\", \"bias\"]],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    apply_embedding_weights(\n",
    "        target_layer=model.get_layer(\"Transformer/posembed_input\"),\n",
    "        source_weights=params_dict[\"Transformer/posembed_input/pos_embedding\"],\n",
    "    )\n",
    "    source_keys_used.append(\"Transformer/posembed_input/pos_embedding\")\n",
    "    for match in matches:\n",
    "        source_keys_used.extend(match[\"keys\"])\n",
    "        source_weights = [params_dict[k] for k in match[\"keys\"]]\n",
    "        if match.get(\"reshape\", False):\n",
    "            source_weights = [\n",
    "                source.reshape(expected.shape)\n",
    "                for source, expected in zip(\n",
    "                    source_weights, match[\"layer\"].get_weights()\n",
    "                )\n",
    "            ]\n",
    "        match[\"layer\"].set_weights(source_weights)\n",
    "    unused = set(source_keys).difference(source_keys_used)\n",
    "    if unused:\n",
    "        warnings.warn(f\"Did not use the following weights: {unused}\", UserWarning)\n",
    "    target_keys_set = len(source_keys_used)\n",
    "    target_keys_all = len(model.weights)\n",
    "    if target_keys_set < target_keys_all:\n",
    "        warnings.warn(\n",
    "            f\"Only set {target_keys_set} of {target_keys_all} weights.\", UserWarning\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_B = {\n",
    "    \"dropout\": 0.1,\n",
    "    \"mlp_dim\": 3072,\n",
    "    \"num_heads\": 12,\n",
    "    \"num_layers\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "}\n",
    "\n",
    "CONFIG_L = {\n",
    "    \"dropout\": 0.1,\n",
    "    \"mlp_dim\": 4096,\n",
    "    \"num_heads\": 16,\n",
    "    \"num_layers\": 24,\n",
    "    \"hidden_size\": 1024,\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://github.com/faustomorales/vit-keras/releases/download/dl\"\n",
    "WEIGHTS = {\"imagenet21k\": 21_843, \"imagenet21k+imagenet2012\": 1_000}\n",
    "SIZES = {\"B_16\", \"B_32\", \"L_16\", \"L_32\"}\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    image_size: int,\n",
    "    patch_size: int,\n",
    "    num_layers: int,\n",
    "    hidden_size: int,\n",
    "    num_heads: int,\n",
    "    name: str,\n",
    "    mlp_dim: int,\n",
    "    dropout=0.1,\n",
    "    activation=\"linear\",\n",
    "    representation_size=None,\n",
    "):\n",
    "    \"\"\"Build a ViT model.\n",
    "\n",
    "    Args:\n",
    "        image_size: The size of input images.\n",
    "        patch_size: The size of each patch (must fit evenly in image_size)\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "        num_layers: The number of transformer layers to use.\n",
    "        hidden_size: The number of filters to use\n",
    "        num_heads: The number of transformer heads\n",
    "        mlp_dim: The number of dimensions for the MLP output in the transformers.\n",
    "        dropout_rate: fraction of the units to drop for dense layers.\n",
    "        activation: The activation to use for the final layer.\n",
    "        include_top: Whether to include the final classification layer. If not,\n",
    "            the output will have dimensions (batch_size, hidden_size).\n",
    "        representation_size: The size of the representation prior to the\n",
    "            classification layer. If None, no Dense layer is inserted.\n",
    "    \"\"\"\n",
    "    assert image_size % patch_size == 0, \"image_size must be a multiple of patch_size\"\n",
    "    x = tf.keras.layers.Input(shape=(image_size, image_size, 3))\n",
    "    y = tf.keras.layers.Conv2D(\n",
    "        filters=hidden_size,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        padding=\"valid\",\n",
    "        name=\"embedding\",\n",
    "    )(x)\n",
    "    y = tf.keras.layers.Reshape((y.shape[1] * y.shape[2], hidden_size))(y)\n",
    "    y = layers.ClassToken(name=\"class_token\")(y)\n",
    "    y = layers.AddPositionEmbs(name=\"Transformer/posembed_input\")(y)\n",
    "    for n in range(num_layers):\n",
    "        y, _ = layers.TransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            dropout=dropout,\n",
    "            name=f\"Transformer/encoderblock_{n}\",\n",
    "        )(y)\n",
    "    y = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"Transformer/encoder_norm\")(y)\n",
    "    y = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(y)\n",
    "    if representation_size is not None:\n",
    "        y = tf.keras.layers.Dense(representation_size, name=\"pre_logits\", activation=\"tanh\")(y)\n",
    "    return tf.keras.models.Model(inputs=x, outputs=y, name=name)\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained(size, weights, pretrained_top, model):\n",
    "    \"\"\"Load model weights for a known configuration.\"\"\"\n",
    "    fname = f\"ViT-{size}_{weights}.npz\"\n",
    "    origin = f\"{BASE_URL}/{fname}\"\n",
    "    local_filepath = tf.keras.utils.get_file(fname, origin, cache_subdir=\"weights\")\n",
    "    load_weights_numpy(model, local_filepath, pretrained_top)\n",
    "\n",
    "    \n",
    "\n",
    "def vit_b16(\n",
    "    image_size: int = 224,\n",
    "    classes=1000,\n",
    "    activation=\"linear\",\n",
    "    pretrained=True,\n",
    "    weights=\"imagenet21k+imagenet2012\",\n",
    "):\n",
    "    \"\"\"Build ViT-B16. All arguments passed to build_model.\"\"\"\n",
    "    model = build_model(\n",
    "        **CONFIG_B,\n",
    "        name=\"vit-b16\",\n",
    "        patch_size=16,\n",
    "        image_size=image_size,\n",
    "        activation=activation,\n",
    "        representation_size=768 if weights == \"imagenet21k\" else None,\n",
    "    )\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            size=\"B_16\", weights=weights, model=model, pretrained_top=False\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_l16(\n",
    "    image_size: int = 384,\n",
    "    classes=1000,\n",
    "    activation=\"linear\",\n",
    "    pretrained=True,\n",
    "    weights=\"imagenet21k+imagenet2012\",\n",
    "):\n",
    "    \"\"\"Build ViT-L16. All arguments passed to build_model.\"\"\"\n",
    "    model = build_model(\n",
    "        **CONFIG_L,\n",
    "        patch_size=16,\n",
    "        name=\"vit-l16\",\n",
    "        image_size=image_size,\n",
    "        activation=activation,\n",
    "        representation_size=1024 if weights == \"imagenet21k\" else None,\n",
    "    )\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            size=\"L_16\", weights=weights, model=model, pretrained_top=False\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee99819",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_l16(\n",
    "    image_size=384,\n",
    "    pretrained=True,\n",
    "    weights=\"imagenet21k+imagenet2012\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, dpi=64, show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60770c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights/vit_l16_imagenet21k_imagenet2012.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271d883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
